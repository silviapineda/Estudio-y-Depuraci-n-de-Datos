---
cover: >-
  ../../.gitbook/assets/Introducing-Automated-Time-Series-Anomaly-Detection_blog_Image_v.1.0.webp
coverY: 0
---

# Detecci√≥n y tratamiento de datos at√≠picos (multivariante)

## Enfoque Multivariante

Un estudio multivariante es aquel que estudia m√°s de dos variables de forma simultanea. Su objetivo principal es **capturar las relaciones internas** (correlaciones, dependencias, estructuras latentes) que no se aprecian en un enfoque univariado o bivariado.&#x20;

Al igual que tenemos m√©todos m√°s sofisticados de _machine learning_ para analizar nuestros datos, los tenemos tambi√©n para la detecci√≥n de datos at√≠picos.&#x20;

Los outliers **no siempre destacan en la distribuci√≥n global**. En un espacio multidimensional puede haber puntos perfectamente ‚Äúnormales‚Äù vistos variable a variable, pero **raros respecto a sus vecinos m√°s pr√≥ximos**. LOF (Local Outlier Factor) ‚Äîpropuesto por [Breunig et al., 2000](https://dl.acm.org/doi/10.1145/335191.335388)‚Äî aborda ese problema midiendo la _desviaci√≥n local_.

### Local Outlier Factor (LOF)

LOF es un algoritmo de detecci√≥n de valores at√≠picos basado en la proximida y la densidad. Eval√∫a la densidad local de los puntos de datos para identificar valores at√≠picos. A diferencia de algoritmos de agrupaci√≥n como k-Nearest Neighbors (KNN), LOF se centra en encontrar observaciones que se desv√≠en del patr√≥n de densidad local. Es decir, modela los valores at√≠picos como puntos que est√°n aislados de los datos restantes.

En la pr√°ctica, la densidad local se obtiene de los k vecinos m√°s cercanos. La puntuaci√≥n LOF de una observaci√≥n es igual a la relaci√≥n entre la densidad local promedio de sus k vecinos m√°s cercanos y su propia densidad local. Se espera que un _caso_ normal tenga una densidad local similar a la de sus vecinos, mientras que los _casos_ an√≥malos se espera que tengan una densidad local mucho menor.

En un **KNN**, lo que hacemos es clasificar a un nuevo miembro a una _clase_ o categor√≠a que ya existe. Lo determina seg√∫n que sus atributos se parezcan a los casos de una categor√≠a u otra. Supongamos los grupos que se presentan en la siguiente gr√°fica. Se quiere asignar un nuevo punto, en negro. KNN buscar√° los _k_ puntos m√°s cercanos a √©ste para encontrar la clase dominante del grupo. Para garantizar la existencia de una clase dominante, k debe ser un n√∫mero impar.\


<figure><img src="../../.gitbook/assets/image (15).png" alt="" width="375"><figcaption></figcaption></figure>

En este ejemplo se utilizaron las cinco observaciones m√°s cercanas ($$ùêæ=5$$), para determinar a qu√© grupo asignar al nuevo _individuo_. Hay muchas maneras de _medir distancias_. Cada manera se adapta de mejor o peor manera al tipo de atributos con que cuentan los individuos o casos. Esto lo estudiar√©is en asignaturas m√°s adelante.&#x20;

**El algoritmo LOF** usa una idea similar al KNN, pero orientado a detectar datos an√≥malos. Calcula una puntuaci√≥n (denominada factor de valor at√≠pico local) que refleja el grado de anomal√≠a de las observaciones. Mide la desviaci√≥n de la _densidad local_ de un punto con respecto a sus vecinos. La idea es detectar las muestras que tienen una densidad sustancialmente menor que sus vecinas.

En la pr√°ctica, la densidad local se obtiene de los k vecinos m√°s cercanos. La puntuaci√≥n LOF de una observaci√≥n es igual a la relaci√≥n entre la densidad local promedio de sus k vecinos m√°s cercanos y su propia densidad local. Se espera que un _caso_ normal tenga una densidad local similar a la de sus vecinos, mientras que los _casos_ an√≥malos se espera que tengan una densidad local mucho menor.

**Resumen del Algoritmo:** Aqu√≠ ten√©is un esquema b√°sico del algoritmo LOF:

* Para cada punto de datos:
  * Calcula la distancia a sus k vecinos m√°s cercanos.&#x20;
  * Eval√∫a la densidad local en funci√≥n de la distancia promedio a los vecinos.
  * Compara la densidad local del punto con las densidades locales de sus vecinos.
  * Calcula el Local Outlier Factor (LOF) como la relaci√≥n entre la densidad del punto y la densidad promedio de sus vecinos.
* **Elegir&#x20;**_**k**_ (n√∫mero de vecinos ‚Äúcercanos‚Äù): $$k = log(n)$$
* Para cada observaci√≥n **A**, calcular su **k-distance(A)** como‚ÄÜla distancia al k-√©simo vecino.
* Derivar la **reachability distance** respecto a cada vecino **B:**

&#x20;        $$\operatorname{reach}\_{k}(A,B)=\max\{\text{k-distance}(B),\;d(A,B)\}$$



* Obtener la **densidad de alcance local** (_local reachability density_, LRD):$$\text{LRD}_{k}(A)=\Big(\frac{\sum_{B\in N_{k}(A)} \operatorname{reach}_{k}(A,B)}{|N_{k}(A)|}\Big)^{-1}$$



*   Comparar densidades:&#x20;

    &#x20;$$\text{LOF}_{k}(A)=\frac{\sum_{B\in N_{k}(A)} \text{LRD}_{k}(B)}{|N_{k}(A)|}\;\Big/\;\text{LRD}_{k}(A)$$

**Interpretaci√≥n:**

* **LOF > 1:** Un valor LOF mayor que 1 indica que el punto tiene una densidad local significativamente menor que la de sus vecinos. En otras palabras, el punto se encuentra en una regi√≥n donde la densidad de puntos es menor de lo esperado en comparaci√≥n con su entorno local. Esto sugiere que el punto podr√≠a ser un valor at√≠pico.
* **LOF ‚âà 1:** Un valor LOF cercano a 1 indica que el punto tiene una densidad local similar a la de sus vecinos. En este caso, el punto se encuentra en una regi√≥n donde la densidad de puntos es consistente con su entorno local. No se considera un valor at√≠pico seg√∫n LOF.
* **LOF < 1:** Un valor LOF menor que 1 indica que el punto tiene una densidad local m√°s alta que la de sus vecinos. Esto puede suceder en regiones de alta densidad donde el punto est√° m√°s densamente rodeado que la media de sus vecinos. En este caso, el punto puede considerarse menos at√≠pico que sus vecinos.

La costumbre lleva a considerar an√≥malos a **valores de LOF por encima de** $$1.5$$ **√≥** $$2$$**,** pero cada problema debe definir cu√°l es su umbral adecuado.

**Ejemplo con la base de datos de R iris**

El conjunto de datos Iris consta de 150 observaciones de iris, con 50 observaciones de cada una de las tres especies de iris: setosa, versicolor y virginica. Para cada observaci√≥n, se miden cuatro caracter√≠sticas: longitud del s√©palo, ancho del s√©palo, longitud del p√©talo y ancho del p√©talo. Es conocido por su utilidad en la demostraci√≥n de t√©cnicas de clasificaci√≥n y an√°lisis estad√≠stico.

```r
#Instala y carga los paquetes
install.packages(c("dbscan", "class"))

library(dbscan)
library(class)
library(ggplot2)

# Carga el conjunto de datos iris
data(iris)


# A√±ade un valor at√≠pico al conjunto de datos
iris$Species<-as.character(iris$Species)
iris_atipico <- rbind(iris, c(7, 5, 5, 0.7, "Atipico"))
iris_atipico$Species<-as.factor(iris_atipico$Species)
iris_atipico[,c(1:4)]<-lapply(iris_atipico[,c(1:4)],as.numeric)
iris$Species<-as.factor(iris$Species)


# Gr√°fico de dispersi√≥n para mostrar el conjunto de datos con un valor at√≠pico
ggplot(iris_atipico, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  geom_text(data = iris_atipico[nrow(iris_atipico),], aes(label = "Atipico"), vjust = -1) +
  labs(title = "Conjunto de Datos Iris con un Valor At√≠pico",    
       x = "Longitud del S√©palo", y = "Anchura del S√©palo")

ggplot(iris_atipico, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() +
  geom_text(data = iris_atipico[nrow(iris_atipico),], aes(label = "Atipico"), vjust = -1) +
  labs(title = "Conjunto de Datos Iris con un Valor At√≠pico",
       x = "Longitud del P√©talo", y = "Anchura del P√©talo")

```

<figure><img src="../../.gitbook/assets/image (189).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (259).png" alt=""><figcaption></figcaption></figure>

```r
# Aplica LOF para detecci√≥n de valores at√≠picos
k<-round(log(nrow(iris_atipico)))
lof_resultados <- lof(iris_atipico[, 1:4],minPts = k)

iris_atipico$lof<-lof_resultados
ggplot(iris_atipico, aes(y = lof)) +
  geom_boxplot(fill = "skyblue", outlier.color = "red", outlier.shape = 16) +
  theme_minimal() +
  labs(title = "Distribuci√≥n de LOF Scores")
  
### Los que tienen un LOF mayor de 1.5
cbind(iris_atipico[which(iris_atipico$lof>1.5),])

# Gr√°fico de dispersi√≥n con colores seg√∫n las puntuaciones LOF
ggplot(iris_atipico, aes(x = Sepal.Length, y = Sepal.Width, colour = lof_resultados)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red", name = "LOF Score") +
  labs(title = "Detecci√≥n de Valores At√≠picos con LOF",
       x = "Longitud del S√©palo", y = "Anchura del S√©palo")
       
ggplot(iris_atipico, aes(x = Petal.Length, y = Petal.Width, colour = lof_resultados)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red", name = "LOF Score") +
  labs(title = "Detecci√≥n de Valores At√≠picos con LOF",
       x = "Longitud del P√©talo", y = "Anchura del P√©talo")

```

```r
Sepal.Length Sepal.Width Petal.Length Petal.Width    Species      lof
21           5.4         3.4          1.7         0.2     setosa 1.590261
23           4.6         3.6          1.0         0.2     setosa 2.107731
24           5.1         3.3          1.7         0.5     setosa 1.510867
32           5.4         3.4          1.5         0.4     setosa 1.529246
42           4.5         2.3          1.3         0.3     setosa 2.406485
63           6.0         2.2          4.0         1.0 versicolor 1.717688
107          4.9         2.5          4.5         1.7  virginica 1.992299
110          7.2         3.6          6.1         2.5  virginica 1.840244
151          7.0         5.0          5.0         0.7    Atipico 5.176055
```

<figure><img src="../../.gitbook/assets/image (190).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (260).png" alt=""><figcaption></figcaption></figure>

En el gr√°fico vemos claramente el valor at√≠pico con un lof muy alto y en la tabla, donde hemos pintado todos los datos que est√°n por encima de lof score > 1.5 vemos como el dato at√≠pico tiene un claro valor que se va del reso con un lof score = 5.17. En este caso no tenemos que borrar todos los valores que est√©n por encima de 1.5 sino ser capaces de identificar que ese valor dista mucho del resto tanto en valor como en el gr√°fico. Adem√°s en este caso vemos que los valores son raros para casi todas las variables, por tanto podemos sospechar que lo que deber√≠amos borrar es la observaci√≥n entera.&#x20;
